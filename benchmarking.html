<head>
  <title>Will Sims</title>
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <link rel="stylesheet" href="assets/style.css">
  <script src="https://use.fontawesome.com/f3e3497967.js"></script>
  <link rel="apple-touch-icon" sizes="57x57" href="assets/favicon/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="assets/favicon/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="assets/favicon/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="assets/favicon/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="assets/favicon/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="assets/favicon/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="assets/favicon/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="assets/favicon/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="assets/favicon/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="assets/favicon/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="assets/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="assets/favicon/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="assets/favicon/favicon-16x16.png">
  <link rel="manifest" href="assets/favicon/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="assets/favicon/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
</head>

<html>
    <body>
      <header class = "header">
        <div class="wrapper">
          <div class = "nav">
              <a href="WillSimsResume.pdf">Resume</a>
              <a href="http://wrsims.com">Work</a>
          </div>
          <div class="logo">
            <a href="http://wrsims.com">Will Sims</a>
          </div>
        </div>
      </header>

      <div class="project-overview">
        <div class = "projectWrapper">
          <h3>Puppet Usability Benchmarking</h3>
          <div class="project-info project-challenge">
  			      <h4>The Challenge</h4>
                <p>Over the summer I worked as a UX intern at <a href="https://puppet.com/">Puppet</a>. Puppet is a company makes IT automation software for managing and securing your infrastructure.</p>
                <p>My main intern project was to develop and conduct a usability benchmarking study for measuring the ease of use of Puppet products.</p> </div>

          <div class="project-info project-role">
    			     <h4>Goals</h4>
                    <ul>
                      <li>Research and develop a methodology for measuring the overall ease of use of Puppet Enterprise.</li>
                      <li>Measure the current usability of Puppet Enterprise to set a quantitative baseline for tracking the performance of each release.</li>
                      <li>Discover areas for improvement and specific usability issues through user testing.</li>
                    </ul>
          </div>
        </div>
        <div class = "projectWrapper">
          <h4>Methodology</h4>
            <p><strong>Metrics:</strong> I used common usability metrics such as <strong>completion rates, task times, errors and satisfaction</strong> to measure the overall ease of use of Puppet Enterprise. Error scores were calculated by defining the number of error opportunities before each task and then comparing that value with the number of errors that occurred during testing. Satisfaction was measured using the Single Ease Question (SEQ) which is 7-point rating scale to assess how difficult users find a task. All of this metrics were combined into a single usability metric (SUM) score which was used to communicate overall performance.</p>
            <p>Example of the spreadsheet from <a href="http://www.measuringu.com/SUM/index.htm">MeasuringU</a> that I used to record my results.</p>
            <img style='width: 75%; margin: auto; object-fit: contain' src="assets/sum_spreadsheet.png"/>
            <p><strong>Tasks:</strong> To effectively benchmarking an entire product, users should perform tasks that are high level and accommodate UI changes so you can continuing tracking the usability of core features with each release. Myself and two other UX team members designed three core tasks which provided a framework for testing usability benchmarking with the SUM. We created job stories that describe functionality most important to our users and then designed tasks that accomplished those user goals.</p>
            <p><strong>Users:</strong> I facilitated 5 moderated usability benchmarking sessions with highly technical users that were using Puppet to manage their infrastructure at their organization. Users were selected based on their self-reported expertise with the product and the version they were running. The goal was to select users that were familiar with newer versions of Puppet, but also represented the average consumer.</p>
          <h4>Results</h4>
            <p><strong>Hurdles:</strong> One issue we found during testing is that error scores did not accurately represent the usability of a task due to the complexity of Puppet Enterprise and number of potential paths.</p>
            <p>If a participant chooses the wrong path, then every subsequent action creates a new error opportunity. Since the number of error opportunities directly influences SUM, scores may be inflated because we happened to record a large number of error opportunities for a certain task. Due to this problem, I made the decision to remove errors from the SUM calculation because it didn’t make sense for such a complex product. That being said, it is still very beneficial to record errors that participants encounter because it targets specific usability issues.</p>
            <p><strong>Specific Usability Issues:</strong> Due to my non-disclosure agreement, I cannot share the specific scores that were obtained during the usability benchmarking sessions. However, since the version of Puppet that was used for testing is now live I can share some of the usability issues encountered that led to tickets being filed:</p>
            <p>When installing, nearly all users thought that the Puppet master hostname would used automatically if the textbox was left blank. As a result, users attempted to continue installation and were shown the notification below.</p>
            <img style='width: 75%; margin: auto; object-fit: contain' src="assets/puppet_fqdn.png"/>
            <p>A potential solution that was included in the ticket description was to pre-populate the textbox with the domain name since there is only one valid hostname for the installation.</p>
            <p>After installing a module as part of one of the tasks, users needed to refresh their class definitions in order to use the module. There is nothing in the UI to indicate that class definitions need to be refreshed so this step was overlooked.</p>
            <img style='width: 75%; margin: auto; object-fit: contain' src="assets/puppet_class_defs.png"/>
            <p>The solution that I recommended was to refresh class definitions automatically whenever code is deployed to install a module.</p>
            <p><strong>Future Iterations:</strong> Unmoderated testing appeared to be the most promising method for continuing benchmarking in the future because you are able to gather more data by measuring participants without dedicated researchers, you can test many different areas of the product by creating short tasks that only involve a specific part of the workflow, and you can eliminate participant fatigue by reducing each test session to 15 minutes.</p>
            <p>The results also showed that the tasks should be modified to be more independent of one another so failing one task doesn’t affect your performance on the next. I suggested that it may be beneficial to look into users completing tasks using their personal infrastructures as a testing environment.</p>
            <h4>Takeaways</h4>
            <p>The highlight of this research was that usability benchmarking with the Single Usability Metric was effective for measuring how well users can complete workflows in Puppet Enterprise and the results provided a reference point that can be measured against in the future.</p>
            <p>Overall, I was happy with the results of the study and the UX team at Puppet will continue to use methodology I developed for usability benchmarking in the future. I learned a great deal about how to take an entire product and split it up into parts that can be measured to provide the most customer value. Through testing and collaboration with members of the UX team, I was able to iterate on areas of my benchmarking methodology to provide a better solution.</p>
        </div>
      </div>
    </div>
      <footer class = "footer">
        <div class = "wrapperFooter">
            <div class = "copyright">
              <h5>© 2016 Will Sims, all rights reserved.</h5>
            </div>
            <div class = "links">
              <a href="mailto:simsw@oregonstate.edu" alt="Email">
                <i class="fa fa-envelope"></i>
              </a>
              <a href="https://www.linkedin.com/in/will-sims-18b318bb" alt="LinkedIn">
                <i class="fa fa-linkedin"></i>
              </a>
              <a href="https://www.github.com/wsims/" alt="GitHub">
                <i class="fa fa-github"></i>
              </a>
              <a href="https://open.spotify.com/user/129745423" alt="Spotify">
                <i class="fa fa-spotify"></i>
            </div>
        </div>
      </footer>
    </body>
</html>
